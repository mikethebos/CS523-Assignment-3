\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Using Genetic Algorithms to Explore Germinal Center Mutation Strategies}
% {\footnotesize Project for Complex Adaptive Systems }
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\author{\IEEEauthorblockN{Connor Frost}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of New Mexico}\\
Albuquerque, United States \\
frostc@unm.edu}
\and
\IEEEauthorblockN{Craig Parry}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of New Mexico}\\
Albuquerque, United States \\
parryc@unm.edu}
\and
\IEEEauthorblockN{Michael Adams}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{University of New Mexico}\\
Albuquerque, United States \\
mikethebos@unm.edu}
}

\maketitle

\begin{abstract}

% Things to address
%   - We would like to explore cellular automata, specifically Neural Cellular Automata(NCA) and compare our results to traditional cellular automata. We have found an interesting thread on Differential Self Organizing Systems[1], which we are going to use for the basis of our exploration. Our work will include addressing the following components with respect to neural cellular automata:
% 
% 

Our project explores the recent innovations of Neural Cellular Automata's with relation to different datasets. Neural Cellular Automata (NCAs) leverage a deep learning network to both perceive and potentially react to its surrounding in a way that is similar to traditional CAs. By leveraging deep learning networks, this allows a non-discrete solution to CA rules and easier allows the NCAs to learn from well-established supervised datasets. 

This exploration with varying MNIST (Modified National Institute of Standards and Technology) datasets help us compare the results to state of the art results achieved by other models. This offers unique insights into the classification abilities of NCAs and the various problems that can be solved by this emerging technology.

\end{abstract}


\section{Introduction}

In traditional Cellular Automata (CA), there consists of a n-dimensional grid of cells where each cell consists of an on or off state. At each iteration, the grid is processed according to the ruleset and a new grid is generated. This iterative process of calculating the next iteration leads to interesting results. The most famous example of a 2D cellular automata is considered to be Conway's Game of Life, where a player creates an input and the evolutionary ruleset determines the output of the game. In neural cellular automata, the 3x3 convolution layer takes the place of a window for each grid cell. As seen in Fig 2, continuous-valued convolutions provide each cell the information about its neighborhood needed to update itself. A NCA's Convolutions provide two new abilities; They allow a cell's state to be in the range of continuous real numbers rather than discrete values, and any differentiable function can be used and will thus yield more sophisticated behavior than a regular CA. Theoretically, this has the ability to solve more complex problems. 

In terms of the cost of computation, neural networks and cellular automata can both take advantage of parallelism. However, neural networks are said to be expensive since high end graphics processing units (GPUs) are generally needed to perform training in a reasonable amount of time. This is due to the complexity involved in performing back-propagation using gradient descent during supervised learning.

In our project we adapt and extend the code developed by the self-organizing NCA series \cite{selforg}, to accomplish these classification and generative approaches to data. We also look adversarial approaches and the overall cost of computation.


% Things to address
%   - Introduce Cellular Automata and the related Neural CAs that are proposed
%   - Discuss what makes NCAs more better by using Deep Learning
%   - Fundamentals of this stuff is based on deep learning where gradients are propagated and etc. Probably a simple graph of the CNNs used.
%   - Probably a few formulas as well for consistency.

\section{Methods & Results}

\subsection{Overview of Fashion MNIST}

The Fashion MNIST dataset is considered a clothing and accessory dataset consisting of ten classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot. The dataset consists of 60,000 training images, and 10,000 validation images, all of which are 28x28 pixels grayscale images. 

\begin{figure}[htbp]
\centering{\includegraphics[scale = .4]{resources/fashion_mnist.png}}
\caption{ Visualization of a subset of images from the Fashion MNIST dataset that is trained upon, in order from top to bottom: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot } 
\end{figure}

The current state of the art results for the models is \%96.91 achieved by the DARTS model \cite{Darts}. 

\section{Data Preprocessing}

There is fundamentally an innate segmentation task of partitioning "alive" cells (where the NCAs would be calculated) and the dead cells. In images, the alive cells would be defined by the pixels of the object subject to classification excluding the background. We do much of this data preprocessing by filtering out the background this is typically the darkest cells in the image, and manually calculate the alive cell mask.


\section{Fashion MNIST Results}

\begin{figure}[htbp]
\centering{\includegraphics[scale = .4]{resources/test_losspng.png}}
\caption{Three dimensional box plot of the mean hamming distance between the adjusted probabilities on an }
\end{figure}


\begin{figure}[htbp]
\centering{\includegraphics[scale = .4]{resources/test_accuracy.png}}
\caption{Three dimensional box plot of the mean hamming distance between the adjusted probabilities on an }
\end{figure}

\begin{figure}[htbp]
\centering{\includegraphics[scale = .4]{resources/test_agreement.png}}
\caption{Three dimensional box plot of the mean hamming distance between the adjusted probabilities on an }
\end{figure}

\section{Emulating Germinal Center Capabilities with DEAP}



\section{Discussion}

\subsubsection{Conclusions}


\begin{thebibliography}{00}

\bibitem{self-classify} Randazzo, et al., "Self-classifying MNIST Digits", Distill, 2020.

\bibitem{adversarial} Randazzo, et al., "Adversarial Reprogramming of Neural Cellular Automata", Distill, 2021.

\bibitem{Darts} M. S. Tanveer, M. U. K. Khan, και C.-M. Kyung, ‘Fine-Tuning DARTS for Image Classification’, CoRR, τ. abs/2006.09042, 2020.

\end{thebibliography}

\section*{Contribution Statements}

\subsection{Connor} 

\subsection{Craig} 

\subsection{Michael} 


\end{document}


